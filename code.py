# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_q-nG5iEAUcE9UXIraW6lNNbDQ5oyClQ
"""

from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pickle
import sys
import pandas as pd
import numpy as np
from tensorflow import keras
import librosa
import librosa.display
# !unzip \m4-four-datasets.zip 
loaded_model = keras.models.load_model('content/speach emotion model/m4-four-datasets')



from pydub import AudioSegment
from pydub.utils import make_chunks

# myaudio = AudioSegment.from_file("myAudio.wav" , "wav") 

sound_file = AudioSegment.from_mp3(sys.argv[1])


type(sound_file)

chunk_length_ms = 10000 # pydub calculates in millisec
chunks = make_chunks(sound_file, chunk_length_ms) #Make chunks of one sec

# sound_file = AudioSegment.from_wav("audio 1.wav")
# audio_chunks = split_on_silence(sound_file, min_silence_len=1000, silence_thresh=-40 )
 
# for i, chunk in enumerate(audio_chunks):
#    out_file = "chunk{0}.wav".format(i)
#    print("exporting", out_file)
#    chunk.export(out_file, format="wav")

def noise(data):
    noise_amp = 0.035*np.random.uniform()*np.amax(data)
    data = data + noise_amp*np.random.normal(size=data.shape[0])
    return data

def stretch(data, rate=0.8):
    return librosa.effects.time_stretch(data, rate)

def shift(data):
    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)
    return np.roll(data, shift_range)

def pitch(data, sampling_rate, pitch_factor=0.7):
    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)

def extract_features(data, sample_rate):
    # ZCR
    result = np.array([])
    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)
    result=np.hstack((result, zcr)) # stacking horizontally

    # Chroma_stft
    stft = np.abs(librosa.stft(data))
    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)
    result = np.hstack((result, chroma_stft)) # stacking horizontally

    # MFCC
    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)
    result = np.hstack((result, mfcc)) # stacking horizontally

    # Root Mean Square Value
    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)
    result = np.hstack((result, rms)) # stacking horizontally

    # MelSpectogram
    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)
    result = np.hstack((result, mel)) # stacking horizontally
    
    return result

def get_features(data, sample_rate):
    # duration and offset 6are used to take care of the no audio in start and the ending of each audio files as seen above.
    # data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)
    # data = 
    # print(type(data))
    # print(type(sample_rate))
    # without augmentation
    res1 = extract_features(data, sample_rate)
    result = np.array(res1)
    
    # data with noise
    noise_data = noise(data)
    res2 = extract_features(noise_data, sample_rate)
    result = np.vstack((result, res2)) # stacking vertically
    
    # data with stretching and pitching
    new_data = stretch(data)
    data_stretch_pitch = pitch(new_data, sample_rate)
    res3 = extract_features(data_stretch_pitch, sample_rate)
    result = np.vstack((result, res3)) # stacking vertically
    
    return result

def audiosegment_to_librosawav(audiosegment):    
    channel_sounds = audiosegment.split_to_mono()
    samples = [s.get_array_of_samples() for s in channel_sounds]

    fp_arr = np.array(samples).T.astype(np.float32)
    fp_arr /= np.iinfo(samples[0].typecode).max
    fp_arr = fp_arr.reshape(-1)

    return fp_arr



def audio_to_labels(chunk):
  # sample_rate = chunk.frame_rate
  feature = get_features(audiosegment_to_librosawav(chunk), chunk.frame_rate)
  xx = []
  temp =[]
  for ele in feature:
      temp.extend(ele)
  xx.append(temp)
  
  Features = pd.DataFrame(xx)
  xx = Features.iloc[: ,:-1].values
  
  # scaler = StandardScaler()
  with open('mean.npy', 'rb') as f:
    mean = np.load(f)
  with open('var.npy', 'rb') as f:
    var = np.load(f)


  # print(mean.shape)
  # print(var.shape)
  # scaler.mean_ = mean
  # scaler.var_ = var
  # scaler.fit_transform(xx)
  # xx = scaler.transform(xx)
  # (xx-mean)/var
  # print(xx.min())
  xx = (xx-xx.min()) / (xx.max() - xx.min())
  # print(xx)
  xx = np.expand_dims(xx, axis=2)
  xx = loaded_model.predict(xx)
  
  with open('encoder (1).pickle', 'rb') as f:
      encoder1 = pickle.load(f)
  return (encoder1.inverse_transform(xx).flatten())



import os
import torch
import torchvision
import torch.nn as nn
from tqdm.notebook import tqdm
import torch.nn.functional as F
import matplotlib.pyplot as plt
from IPython.display import Image
import torchvision.transforms as T
from torch.utils.data import DataLoader
from torchvision.utils import make_grid
from torchvision.utils import save_image
import keras
import matplotlib.pyplot as plt

def get_device():
    if torch.cuda.is_available():
        return torch.device("cuda")
    else:
        return torch.device("cpu")
device = get_device()

latent_size=128
generator = nn.Sequential(
    #in: 128 x 1 x 1
    
    nn.ConvTranspose2d(latent_size, 1024, kernel_size=4, stride=1, padding=0, bias=False),
    nn.BatchNorm2d(1024),
    nn.ReLU(True),
    #128 x 1024 x 4 x 4
    
    nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(512),
    nn.ReLU(True),
    #128 x 512 x 8 x 8
    
    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(256),
    nn.ReLU(True),
    #128 x 256 x 16 x 16
    
    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(128),
    nn.ReLU(True),
    #128 x 128 x 32 x 32
    
    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(64),
    nn.ReLU(True),
    #128 x 64 x 64 x 64
    
    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),
    #128 x 3 x 128 x 128
    nn.Tanh()
    
    
)
generator.load_state_dict(torch.load('model', map_location=device))
# generator = keras.models.load_model('/content/model')

def to_device(data, device):
    if isinstance(data, (list,tuple)):
        return [to_device(x, device) for x in data]
    return data.to(device, non_blocking=True)

generator = to_device(generator, device)

# plt.figure()

# f, axarr = plt.subplots(len(chunks),1) 
a = 0
for clip in chunks:
  label = (audio_to_labels(clip))
  fixed_latent = torch.randn(128, latent_size, 1, 1, device=device)
  tt = generator(fixed_latent*(-1))
  fig = plt.figure()
  fig.suptitle(label[0], fontsize=20)
  # plt.label(label)
  # plt.figure()
  # plt.imshow(image)
  imagee = tt.cpu().detach().numpy()[0].transpose()
  if label == 'angry' or label == 'sad' or label == 'disgust' or label == 'fear':
    imagee /= 2
  elif label == 'happy' or label == 'calm':
    imagee *= 2
  a+=1
  imagee = (imagee-imagee.min()) / (imagee.max()-imagee.min())
  plt.imsave('output/slice' + str(a) + '[' + label[0] + '].jpg', imagee, cmap='gray')
